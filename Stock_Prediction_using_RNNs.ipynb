{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stock Prediction using RNNs","metadata":{}},{"cell_type":"code","source":"#Uncomment to install yfinance when running it the first time\n!pip install yfinance","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:35.739787Z","iopub.execute_input":"2022-04-10T17:53:35.740026Z","iopub.status.idle":"2022-04-10T17:53:46.688994Z","shell.execute_reply.started":"2022-04-10T17:53:35.739963Z","shell.execute_reply":"2022-04-10T17:53:46.688195Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Importing all necessary libraries","metadata":{}},{"cell_type":"code","source":"#Importing all libraries needed for the project.\n\n#numpy is used for operations on arrays\nimport numpy as np\n\n#pandas is used to perform operations on dataframes since yfinance return the data in dataframes. \nimport pandas as pd\n\n#yfinance is used to download the stock data from yahoo finance.\nimport yfinance as yf\n\n#torch is used to create, train and test the neural networks\nimport torch\n\n#sklearn is only used for the mean squared error function.\nimport sklearn\n\n#matplotlib.pyplot is used for plotting the results.\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:46.692669Z","iopub.execute_input":"2022-04-10T17:53:46.692884Z","iopub.status.idle":"2022-04-10T17:53:48.959727Z","shell.execute_reply.started":"2022-04-10T17:53:46.692858Z","shell.execute_reply":"2022-04-10T17:53:48.958962Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Defining all functions needed","metadata":{}},{"cell_type":"code","source":"def download_dataset(stock='AAPL', start='1990-01-01', end='2022-01-01'):\n    \"\"\"This function downloads the stock data needed from yahoo finance in a certain time window.\n    It then performs the necessary steps to calculate the day-to-day gain of the stock chosen from the stock's adjusted close.\n    The stock gain column is then shifted up so as to create the values to predict for each day since we want to predict the next day's gain.\n    The Vix and Nasdaq index are also added to the dataset. Preferably the stock would be listed on the Nasdaq exchange.\n    Returns a pandas dataframe with the stock, vix, nsdq info.\n    \n    Arguments\n    ---------\n    stock : string\n        The ticker of the stock we want to predict.\n    start : string\n        The start date of the dataset. Has to be the format YYYY-MM-DD (example:1990-01-01)\n    end: string\n        The end date of the dataset. Has to be the format YYYY-MM-DD (example:2022-01-01)\n    \n    Returns\n    ---------\n    full_dataset : pandas.core.frame.DataFrame\n        Dataframe containing stock, vix and nasdaq price and volume information.\n    \n    Example\n    --------\n    >>> full_dataset = download_dataset('AMD', '1990-01-01', '2022-01-01')\n    >>> full_dataset.shape\n    >>> (8063, 18)\n    \n    \"\"\"\n    \n    \n    #Downloading stock data to be used\n    stock_df=yf.download(stock, start=start, end=end, progress=True)\n    \n    #Downloading VIX index data to be used. VIX is a volatility index for the market.\n    vix_df=yf.download('^VIX', start=start, end=end, progress=True)\n    \n    #Downloading NASDAQ index data to be used. NASDAQ is a stock exchange mainly for tech stocks.\n    nsdq_df=yf.download('^IXIC', start=start, end=end, progress=True)\n    \n    #Renaming the Columns\n    stock_df.rename(columns={'Open': 'Stock Open', 'High': 'Stock High', 'Low': 'Stock Low', 'Close': 'Stock Close', 'Adj Close': 'Stock Adj Close', 'Volume': 'Stock Volume'}, inplace=True)\n    vix_df.rename(columns={'Open': 'Vix Open', 'High': 'Vix High', 'Low': 'Vix Low', 'Close': 'Vix Close', 'Adj Close': 'Vix Adj Close', 'Volume': 'Vix Volume'}, inplace=True)\n    nsdq_df.rename(columns={'Open': 'Nasdaq Open', 'High': 'Nasdaq High', 'Low': 'Nasdaq Low', 'Close': 'Nasdaq Close', 'Nasdaq Close': 'Nasdaq Adj Close', 'Volume': 'Nasdaq Volume'}, inplace=True)\n    \n    #Adding column for the stock percentage change based on adjusted close dady-over-day. \n    stock_df['Stock Percentage Change'] = stock_df['Stock Adj Close'].pct_change()\n    \n    #Concatenating all dataset into one set.\n    full_dataset=pd.concat([stock_df, vix_df, nsdq_df], axis=1)\n    \n    #Shifting up the stock percentage change. This will be used as the value to predict since we want to predict the change in the stock the following day.\n    full_dataset['Stock Percentage Change'] = full_dataset['Stock Percentage Change'].shift(-1)\n    \n    #Dropping the column for the VIX index because the VIX does not have volume.\n    full_dataset.drop(['Vix Volume'], axis=1, inplace=True)\n    \n    #Removing the last row because the Stock Percentage Change does not exist anymore since we shifted the column up\n    full_dataset = full_dataset[:-1]\n    \n    return full_dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:48.961103Z","iopub.execute_input":"2022-04-10T17:53:48.961991Z","iopub.status.idle":"2022-04-10T17:53:48.974595Z","shell.execute_reply.started":"2022-04-10T17:53:48.961949Z","shell.execute_reply":"2022-04-10T17:53:48.973856Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def create_dataset(full_dataset):\n    \"\"\"This function splits the dataset into X and y dataset consisting of the features and values to predict. \n    For X, this is done by dropping the Stock Percentage Change(value to predict) column from the dataset.\n    For y, this is done by assigning y to be the Stock Percentage Change(value to predict) column from the dataset.\n    \n    Arguments\n    ---------\n    full_dataset : pandas.core.frame.DataFrame\n        Dataframe containing stock, vix and nasdaq price and volume information.\n    \n    Returns\n    ---------\n    X: pandas.core.frame.DataFrame\n        full_dataset dataframe with the Stock Percentage Change(value to predict) column dropped. The remaining columns are the features to train a machine learning model on.\n    y : pandas.core.series.Series\n        Stock Percentage Change(value to predict) column consisting of the values to predict of the dataset.\n    \n    Example\n    --------\n    >>> full_dataset.shape\n    >>> (8063, 18)\n    >>> X, y = create_dataset(full_dataset)\n    >>> X.shape\n    >>> (8063, 17)\n    >>> y.shape\n    >>> (8063,)\n    \n    \"\"\"\n    \n    \n    #Split data into X and y sets.\n    X = full_dataset.drop(['Stock Percentage Change'], axis=1)\n    y = full_dataset['Stock Percentage Change']\n    \n    return X, y","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:48.976620Z","iopub.execute_input":"2022-04-10T17:53:48.976871Z","iopub.status.idle":"2022-04-10T17:53:48.989014Z","shell.execute_reply.started":"2022-04-10T17:53:48.976838Z","shell.execute_reply":"2022-04-10T17:53:48.988142Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def mean_normalization(X, split_percentage=0.8):\n    \"\"\"This function normalizes all features of the dataset before the training/testing split. This is because the training/testing split is done on the timeseries created later.\n    It is easier to normalize the data now rather than on the timeseries and the reults is the same. The mean and standard deviation is calculated on the training set only based on\n    the split_percentage. \n    \n    Arguments\n    ---------\n    X : pandas.core.frame.DataFrame\n        DataFrame containing the dataset of features to train a machine learning model on.\n    split_percentage : float\n        Float representing the percentage of the dataset to use as the training set. \n        Used to determine the point up to where to calculate the mean and standard deviation used for the mean normalization.\n    \n    Returns\n    ---------\n    X_norm : pandas.core.frame.DataFrame\n        Normalized dataset of all features(training and testing).\n    X_norm_mean : pandas.core.series.Series\n        the mean of all features of the training dataset.\n    X_norm_std : pandas.core.series.Series\n        the standard deviation of all features of the training dataset.\n    \n    Example\n    --------\n    >>> X.shape\n    >>> (8063, 17)\n    >>> X_norm, X_norm_mean, X_norm_std = mean_normalization(X, split_percentage)\n    >>> X_norm.shape\n    >>> (8063, 17)\n    >>> X_norm_mean.shape\n    >>> (17,)\n    >>> X_norm_std.shape\n    >>> (17,)\n    \n    \"\"\"\n    \n    #Mean normalizing the training dataset based on the split percentage.\n    norm_midpoint = int(X.shape[0]*split_percentage)\n    X_norm_mean = X[:norm_midpoint].mean()\n    X_norm_std = X[:norm_midpoint].std()\n    X_norm=(X-X_norm_mean)/X_norm_std\n    \n    return X_norm, X_norm_mean, X_norm_std","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:48.990831Z","iopub.execute_input":"2022-04-10T17:53:48.991078Z","iopub.status.idle":"2022-04-10T17:53:49.003480Z","shell.execute_reply.started":"2022-04-10T17:53:48.991046Z","shell.execute_reply":"2022-04-10T17:53:49.002614Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def create_sequences(X_norm, y, window_shape=30):\n    \"\"\"This function creates timeseries of window_shape as datapoints to feed into a RNN using a rolling window of size window_shape. It also converts X and y into numpy arrays.\n    \n    Arguments\n    ---------\n    X_norm : pandas.core.frame.DataFrame\n        Normalized dataset of all features(training and testing).\n    y : pandas.core.series.Series\n        Stock Percentage Change(value to predict) column consisting of the values to predict of the dataset.\n    window_shape : int\n        Integer corresponding to the length of the timeseries used for the prediction. Number corresponding to consecutive trading days.\n    \n    Returns\n    ---------\n    X_sequence : numpy.ndarray\n        Numpy array corresponding to the sequence of window_shape days in the past of all features to train on.\n    y_np : numpy.ndarray\n        Numpy array corresponding to the values to predict of the dataset.\n    \n    Example\n    --------\n    >>> X_norm.shape\n    >>> (8063, 17)\n    >>> type(X_norm)\n    >>> pandas.core.frame.DataFrame\n    >>> y.shape\n    >>> (8063,)\n    >>> type(y)\n    >>> pandas.core.series.Series\n    >>> window_shape = 30\n    >>> X_sequence, y_np = create_sequences(X_norm, y, window_shape)\n    >>> X_sequence.shape\n    >>> (8034, 30, 17)\n    >>> type(X_sequence)\n    >>> numpy.ndarray\n    >>> y_np.shape\n    >>> (8034,)\n    >>> type(y_np)\n    >>> numpy.ndarray\n    \n    \"\"\"\n    \n    from numpy.lib.stride_tricks import sliding_window_view\n    \n    #Convert dataframes to numpy for easy computations\n    X_np = X_norm.to_numpy()\n    y_np = y.to_numpy()\n    \n    #Creating each sequence.\n    X_sequence = sliding_window_view(X_np, window_shape = window_shape, axis=0)\n    \n    #Removing first window_shape-1 values to predict from the dataset because they are not associated to full sequences of features X.\n    y_np = y_np[window_shape-1:]\n    \n    #Swapping the sequence length and the input size for the input into the RNN.\n    X_sequence = np.swapaxes(X_sequence, 1, 2)\n    \n    return X_sequence, y_np","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.004830Z","iopub.execute_input":"2022-04-10T17:53:49.005203Z","iopub.status.idle":"2022-04-10T17:53:49.017241Z","shell.execute_reply.started":"2022-04-10T17:53:49.005162Z","shell.execute_reply":"2022-04-10T17:53:49.016456Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def split_dataset(X, y, split_percentage=0.8, validation_percentage=0.9):\n    \"\"\"This function splits the X and y into the training datasets X_train and y_train and the the testing datasets X_test and y_test. This is based on split_percentage which corresponds to the\n    percentage of the dataset to be used as the training set. Further splits the training set into training and validation set based on the the validation_percentage.\n    \n    Arguments\n    ---------\n    X : pandas.core.frame.DataFrame\n        Numpy array corresponding to the sequence of window_shape days in the past of all features to train on.\n    y : numpy.ndarray\n        Numpy array corresponding to the values to predict of the dataset.\n    split_percentage : float\n        Float representing the percentage of the dataset to use as the training set.\n    validation_percentage : float\n        Float representing the percentage of the training set to be used for training and the rest for validation.\n    \n    Returns\n    ---------\n    X_train : numpy.ndarray\n        Numpy array corresponding to training features.\n    y_train : numpy.ndarray\n        Numpy array corresponding to training values to predict.\n    X_val : numpy.ndarray\n        Numpy array corresponding to validation features.\n    y_val : numpy.ndarray\n        Numpy array corresponding to the validation values to predict.\n    X_test : numpy.ndarray\n        Numpy array corresponding to testing features.\n    y_test : numpy.ndarray\n        Numpy array corresponding to testing values to predict.\n    \n    Example\n    --------\n    >>> X.shape\n    >>> (8034, 17, 30)\n    >>> y.shape\n    >>> (8034,)\n    >>> split_percentage = 0.8\n    >>> validation_percentage = 0.9\n    >>> X_train, y_train, X_val, y_val, X_test, y_test = split_dataset(X, y, split_percentage, validation_percentage)\n    >>> X_train.shape\n    >>> (5784, 30, 17)\n    >>> y_train.shape\n    >>> (5784,)\n    >>> X_val.shape\n    >>> (643, 30, 17)\n    >>> y_val.shape\n    >>> (643,)\n    >>> X_test.shape\n    >>> (1607, 30, 17)\n    >>> y_test.shape\n    >>> (1607,)\n    \n    \"\"\"\n    \n    #Splitting the dataset into training, validation and testing datasets.\n    \n    #Training and Testing split.\n    split_point=int(X.shape[0]*split_percentage)\n    X_train=X[:split_point]\n    X_test=X[split_point:]\n    y_train=y[:split_point]\n    y_test=y[split_point:]\n    \n    #Training and Validation split.\n    val_point = int(X_train.shape[0]*validation_percentage)\n    X_val=X_train[val_point:]\n    X_train=X_train[:val_point]\n    y_val=y_train[val_point:]\n    y_train=y_train[:val_point]\n    \n    return X_train, y_train, X_val, y_val, X_test, y_test","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.018777Z","iopub.execute_input":"2022-04-10T17:53:49.019242Z","iopub.status.idle":"2022-04-10T17:53:49.031523Z","shell.execute_reply.started":"2022-04-10T17:53:49.019207Z","shell.execute_reply":"2022-04-10T17:53:49.030617Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def dataset_to_tensors(X_train, y_train, X_val, y_val, X_test, y_test): \n    \"\"\"This function transforms all X_train, y_train, X_val, y_val, X_test, and y_test into tensors.\n    \n    Arguments\n    ---------\n    X_train : numpy.ndarray\n        Numpy array corresponding to training features.\n    y_train : numpy.ndarray\n        Numpy array corresponding to training values to predict.\n    X_val : numpy.ndarray\n        Numpy array corresponding to validation features.\n    y_val : numpy.ndarray\n        Numpy array corresponding to the validation values to predict.\n    X_test : numpy.ndarray\n        Numpy array corresponding to testing features.\n    y_test : numpy.ndarray\n        Numpy array corresponding to testing values to predict.\n    \n    Returns\n    ---------\n    X_train_t : torch.Tensor\n        Tensor corresponding to training features.\n    X_test_t : torch.Tensor\n        Tensor corresponding to testing features.\n    X_val_t : torch.Tensor\n        Tensor corresponding to validation features.\n    y_val_t : torch.Tensor\n        Tensor corresponding to the validation values to predict.\n    y_train_t : torch.Tensor\n        Tensor corresponding to training values to predict.\n    y_test_t : torch.Tensor\n        Tensor corresponding to testing values to predict.\n    \n    Example\n    --------\n    >>> type(X_train)\n    >>> numpy.ndarray\n    >>> type(X_val)\n    >>> numpy.ndarray\n    >>> type(X_test)\n    >>> numpy.ndarray\n    >>> type(y_train)\n    >>> numpy.ndarray\n    >>> type(y_val)\n    >>> numpy.ndarray\n    >>> type(y_test)\n    >>> numpy.ndarray\n    >>> X_train_t, y_train_t, X_val_t, y_val_t, X_test_t, y_test_t = dataset_to_tensors(X_train, y_train, X_val, y_val, X_test, y_test)\n    >>> type(X_train_t)\n    >>> torch.Tensor\n    >>> type(X_test_t)\n    >>> torch.Tensor\n    >>> type(X_val_t)\n    >>> torch.Tensor\n    >>> type(y_train_t)\n    >>> torch.Tensor\n    >>> type(y_test_t)\n    >>> torch.Tensor\n    >>> type(y_val_t)\n    >>> torch.Tensor\n    \n    \"\"\"\n    \n    #Transforming the training dataset into tensors.\n    X_train_t = torch.tensor(X_train).double()\n    y_train_t = torch.tensor(y_train).double()\n    y_train_t = y_train_t[:, None]\n    \n    #Transforming the validation dataset into tensors.\n    X_val_t = torch.tensor(X_val).double()\n    y_val_t = torch.tensor(y_val).double()\n    y_val_t = y_val_t[:, None]\n    \n    #Transforming the testing dataset into tensors.\n    X_test_t = torch.tensor(X_test).double()\n    y_test_t = torch.tensor(y_test).double()\n    y_test_t = y_test_t[:, None]\n    \n    return X_train_t, y_train_t, X_val_t, y_val_t, X_test_t, y_test_t","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.032881Z","iopub.execute_input":"2022-04-10T17:53:49.033162Z","iopub.status.idle":"2022-04-10T17:53:49.045783Z","shell.execute_reply.started":"2022-04-10T17:53:49.033126Z","shell.execute_reply":"2022-04-10T17:53:49.044940Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#RNN_network class that is used to initialize a RNN model.\nclass RNN_network(torch.nn.Module):\n    def __init__(self, rnn_type = 'gru', input_size=17, hidden_size=5, num_layers=1, dropout=0):\n        super(RNN_network, self).__init__()\n        \"\"\"The initialization method is used to initialize all the components needed to implement the neural model. We have to initialize a series\n        of RNNs and a linear transformation. A choice of GRU or LSTM is given anything else defaults to a regular RNN.\n        Template taken from Lab 5 Exercises for COMP 432 Machine Learning.\n        \n        \"\"\"\n\n        #Initialize the RNN.\n        if rnn_type == 'gru':\n            self.rnn = torch.nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n        elif rnn_type == 'lstm':\n            self.rnn = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        else: \n            self.rnn = torch.nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n\n        #Initialize the Linear transformation.\n        self.linear = torch.nn.Linear(hidden_size, out_features=1) \n        \n        #Introduce Dropout.\n        self.dropout = torch.nn.Dropout(dropout)\n        \n    def forward(self, X):\n        \"\"\"This function processes the input sequence X with a series of RNNs. Then it applies\n        a linear transformation on top of the last hidden state.\n\n        Arguments\n        ---------\n        X : torch.Tensor\n            Tensor containing the sequences (N, L, I).\n\n        Returns\n        ---------\n        out : torch.Tensor\n            Tensor containing the predictions.\n        \n        \"\"\"\n\n        #Run the RNN\n        params, _ = self.rnn(X)\n        Z = self.dropout(params)\n        #Select the last hidden state\n        z = Z[:,-1,:]\n        #Apply linear transformation\n        out = self.linear(z) \n        return out","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.047051Z","iopub.execute_input":"2022-04-10T17:53:49.047299Z","iopub.status.idle":"2022-04-10T17:53:49.059311Z","shell.execute_reply.started":"2022-04-10T17:53:49.047265Z","shell.execute_reply":"2022-04-10T17:53:49.058429Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def train_RNN(X_train_t, y_train_t, rnn_type = 'gru', lr = 0.0005, batch_size = 100, num_epoch = 500, hidden_size = 5, num_layers = 5, dropout = 0.25):\n    \"\"\"This function trains an RNN on the datasets X_train_t and y_train_t based on the passed hyperparameters. \n    Returns the a list of losses computed at for each batch and the trained RNN.\n    Template taken from Lab 5 Exercises for COMP 432 Machine Learning.\n    \n    Arguments\n    ---------\n    rnn_type : string\n        Choice of RNN ('gru', 'lstm' or anything else creates a default rnn).\n    X_train_t : torch.Tensor\n        Tensor corresponding to training features.\n    y_train_t : torch.Tensor\n        Tensor corresponding to training values to predict.\n    lr : float\n        Float corresponding to the learning rate.\n    batch_size : int\n        Integer corresponding to the batch size.\n    num_epoch : int\n        Integer corresponding to the number of epochs of the training.\n    hidden_size : int\n        Integer corresponding the hidden size of the RNN.\n    num_layers : int\n        Number of hidden layers in the RNN.\n    dropout : float\n        Amount of dropout as a percentage. Dropout refers to the number of neurons set to 0 at each layer. Reduces likelihood of overfitting, leads to better generalization.\n    \n    Returns\n    ---------\n    RNN : RNN_network\n        Trained RNN network.\n    losses : list\n        List containing the losses computed for each batch.\n    \n    \"\"\"\n    \n    \n    #Hyperparameters\n    input_size = X_train_t.shape[2]\n    N = X_train_t.shape[0]\n\n    #Storing the losses to plot them\n    losses = []\n\n    #Initialize the RNN \n    rnn = RNN_network(rnn_type = rnn_type, input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout).double()\n    #Compute on GPU if available\n    rnn = rnn.to(device)\n\n    #Initialize the Loss.\n    loss = torch.nn.MSELoss()\n\n    #Initialize the Optimizer.\n    optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n\n    #Training Loop\n    for epoch in range(num_epoch):\n        for i in range(0, N, batch_size):\n\n            #Read minibatches (for both X and y)\n            Xi = X_train_t[i:i+batch_size]\n            yi = y_train_t[i:i+batch_size]\n\n            #Run the model\n            output = rnn(Xi)\n\n            #Compute the loss \n            l = loss(output,yi) \n\n            #Update the parameters\n            rnn.zero_grad()\n            l.backward()\n            optimizer.step()\n\n            #Append losses to the losses list for further analysis.\n            losses.append(l.item())\n\n        #Print loss\n        if (epoch+1) % 5 == 0:\n            print(\"Epoch %03d: Train_loss: %.8f \" %(epoch+1, l.item()))\n            \n    return rnn, losses","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-10T17:53:49.063281Z","iopub.execute_input":"2022-04-10T17:53:49.063752Z","iopub.status.idle":"2022-04-10T17:53:49.075956Z","shell.execute_reply.started":"2022-04-10T17:53:49.063722Z","shell.execute_reply":"2022-04-10T17:53:49.075295Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def predict(rnn, X_test_t):\n    \"\"\"This function uses an RNN to make prediction on a dataset X_test_t.\n    \n    Arguments\n    ---------\n    X_test_t : torch.Tensor\n        Tensor corresponding to testing features.\n    rnn : RNN_network\n        Trained RNN network.\n    \n    Returns\n    ---------\n    y_pred : torch.Tensor\n        The predictions made by the RNN.\n    \n    Example\n    ---------\n    >>> X_test_t.shape\n    >>> torch.Size([1607, 30, 17])\n    >>> y_pred = predict(rnn, X_test_t)\n    >>> y_pred.shape\n    >>> torch.Size([1607, 1])\n    \n    \"\"\"\n    \n    #Put the model in eval mode\n    rnn.eval()\n    #Running the model on the testing set to get the predictions.\n    y_pred=rnn(X_test_t)\n    \n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.077102Z","iopub.execute_input":"2022-04-10T17:53:49.077359Z","iopub.status.idle":"2022-04-10T17:53:49.087242Z","shell.execute_reply.started":"2022-04-10T17:53:49.077324Z","shell.execute_reply":"2022-04-10T17:53:49.086556Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def plot_losses(losses, n):\n    \"\"\"This function plots the losses computed at each batch during the training of an RNN. It smooths the losses by calculation the moving average over n data points.\n    This is done to eliminate outliers to better visualize the data.\n    \n    Arguments\n    ---------\n    losses : list\n        List containing the losses computed for each batch during the training of an RNN.\n    n : int\n        Number of datapoints to use for the moving average.\n    \n    \"\"\"\n    \n    #Takes the losses and computes the moving average over n values.\n    convolved = np.convolve(losses, np.ones(n)/n, mode='valid')\n    #Plots the losses\n    plt.figure(figsize=(15, 10))\n    plt.title('Losses for each batch computed')\n    plt.ylabel('MSE loss')\n    plt.xlabel('')\n    plt.plot(convolved, label=\"losses\")\n    plt.legend","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-10T17:53:49.088681Z","iopub.execute_input":"2022-04-10T17:53:49.088961Z","iopub.status.idle":"2022-04-10T17:53:49.097167Z","shell.execute_reply.started":"2022-04-10T17:53:49.088926Z","shell.execute_reply":"2022-04-10T17:53:49.096368Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def separate_plot_predicted_vs_actual_gains(y_test_t, y_pred):\n    \"\"\"This function plots two lines, on different figures, which correspond to the Predicted day-to-day % gain and the Actual day-to-day % gain.\n    \n    Arguments\n    ---------\n    y_test_t : torch.Tensor\n        Tensor corresponding to testing values to predict.\n    y_pred : torch.Tensor\n        The predictions made by the RNN.\n    \n    \"\"\"\n    \n    #Plotting the Predicted day-to-day %gain.\n    plt.figure(figsize=(15,5))\n    #Set same limits as the plot for the actual gains\n    plt.ylim([y_test_t.cpu().detach().min(), y_test_t.cpu().detach().max()])\n    plt.plot(y_pred.cpu().detach(), label=\"Predicted Gain\")\n    plt.title('Predicted Gain Over Time')\n    plt.xlabel(\"time steps\")\n    plt.ylabel('predicted % gain')\n    plt.legend()\n    \n    #Plotting the Actual day-to-day %gain.\n    plt.figure(figsize=(15,5))\n    plt.plot(y_test_t.cpu().detach(), label=\"Actual Gain\")\n    plt.title('Actual Gain Over Time')\n    plt.xlabel(\"time steps\")\n    plt.ylabel('actual % gain')\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.098659Z","iopub.execute_input":"2022-04-10T17:53:49.098963Z","iopub.status.idle":"2022-04-10T17:53:49.109106Z","shell.execute_reply.started":"2022-04-10T17:53:49.098924Z","shell.execute_reply":"2022-04-10T17:53:49.108352Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def same_plot_predicted_vs_actual_gains(y_test_t, y_pred):\n    \"\"\"This function plots two lines, on the same figure, which correspond to the Predicted day-to-day % gain and the Actual day-to-day % gain.\n    \n    Arguments\n    ---------\n    y_test_t : torch.Tensor\n        Tensor corresponding to testing values to predict.\n    y_pred : torch.Tensor\n        The predictions made by the RNN.\n    \n    \"\"\"\n    \n    #Plotting the Actual day-to-day %gain.\n    plt.figure(figsize=(30,10))\n    plt.plot(y_test_t.cpu().detach(), label=\"Actual Gain\", color='orange')\n    plt.title('Gain Over Time')\n    plt.xlabel(\"time steps\")\n    plt.ylabel('% gain')\n    plt.legend()\n    \n    #Plotting the Predicted day-to-day %gain.\n    plt.plot(y_pred.cpu().detach(), label=\"Predicted Gain\")\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.110072Z","iopub.execute_input":"2022-04-10T17:53:49.111853Z","iopub.status.idle":"2022-04-10T17:53:49.119854Z","shell.execute_reply.started":"2022-04-10T17:53:49.111828Z","shell.execute_reply":"2022-04-10T17:53:49.119008Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def mse(y_test_t, y_pred): \n    \"\"\"This function computes the MSE of the Predicted day-to-day %gain versus the Actual day-to-day %gain.\n    \n    Arguments\n    ---------\n    y_test_t : torch.Tensor\n        Tensor corresponding to testing values to predict.\n    y_pred : torch.Tensor\n        The predictions made by the RNN.\n    \n    Returns\n    ---------\n    mse : numpy.float64\n        Mean squared error of the computed predictions vs the actual % gain.\n    \n    Example\n    ---------\n    >>> y_test_t.shape\n    >>> torch.Size([1607, 1])\n    >>> y_pred = predict(rnn, X_test_t)\n    >>> y_pred.shape\n    >>> torch.Size([1607, 1])\n    \n    \"\"\"\n    \n    #Mean squared error of predictions.\n    from sklearn.metrics import mean_squared_error\n    \n    mse = mean_squared_error(y_test_t.cpu().detach(), y_pred.cpu().detach())\n    \n    return mse","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.121136Z","iopub.execute_input":"2022-04-10T17:53:49.121402Z","iopub.status.idle":"2022-04-10T17:53:49.130149Z","shell.execute_reply.started":"2022-04-10T17:53:49.121364Z","shell.execute_reply":"2022-04-10T17:53:49.129410Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def hyperparameter_3d_search(X_train_t, y_train_t, X_val_t, y_val_t, rnn_type, lr, hidden_size, num_layers):\n    \"\"\"This function performs a hyperparameter search for an RNN on the learning rate, the hidden_size and the number of layers.\n    \n    Arguments\n    ---------\n    X_train_t : torch.Tensor\n        Tensor corresponding to training features.\n    y_train_t : torch.Tensor\n        Tensor corresponding to training values to predict.\n    X_val_t : torch.Tensor\n        Tensor corresponding to validation features.\n    y_val_t : torch.Tensor\n        Tensor corresponding to the validation values to predict.\n    rnn_type : string\n        Choice of RNN ('gru', 'lstm' or anything else creates a default rnn).\n    lr : list\n        List of floats corresponding to the learning rates.\n    hidden_size : list\n        List of integers corresponding the hidden sizes of the RNN.\n    num_layers : list\n        List with the number of hidden layers in the RNN.\n    \n    Returns\n    ---------\n    best_rnn : RNN_network\n        Trained RNN network with the lowest mse on test on the validation set.\n    best_mse : float\n        Lowest mse calculated.\n    best_losses : list\n        losses calculated at each batch of the best RNN.\n    params : list\n        List of dictionaries containing all the hyperparamets tested for each rnn along with their mse. Sorted from lowest to highest mse.\n    \n    \"\"\"\n    \n    #Variables used.\n    current_mse = 0\n    params = []\n    best_mse = 1000\n    best_rnn = None\n    best_losses = []\n    \n    #Hyperparameter search loop.\n    for rate in lr:\n        for size in hidden_size:\n            for num in num_layers:\n                #Train the RNN.\n                rnn, losses = train_RNN(X_train_t, y_train_t, rnn_type = rnn_type, lr = rate, batch_size = 100, num_epoch = 1000, hidden_size = size, num_layers = num, dropout = 0.25)\n                \n                #Performing the prediction on the validation set.\n                y_pred = predict(rnn, X_val_t)\n                \n                #Calculating the mean squared error of the predicted vs actual % gain on the validation set.\n                current_mse = mse(y_val_t, y_pred)\n                \n                #Storing dictionary of parameters and mse.\n                params.append({'mse': current_mse, 'lr': rate, 'hidden_size': size, 'num_layers': num})\n                \n                #Writing the parameters to a file each training loop.\n                with open('params.txt', 'w') as f:\n                    f.write(',\\n'.join(str(i) for i in params))\n                \n                #Testing for the best RNN.\n                if current_mse < best_mse:\n                    best_mse = current_mse\n                    best_rnn = rnn\n                    best_losses = losses\n                    \n    #Sorting the parameters list from best to worst based on mse.               \n    params = sorted(params, key=lambda i: i['mse'])\n    \n    #Writing the sorted parameters to a file at the end of hyperparameter search.\n    with open('sorted_params.txt', 'w') as f:\n        f.write(',\\n'.join(str(i) for i in params))\n                \n    return best_rnn, best_mse, best_losses, params","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.131325Z","iopub.execute_input":"2022-04-10T17:53:49.131953Z","iopub.status.idle":"2022-04-10T17:53:49.144767Z","shell.execute_reply.started":"2022-04-10T17:53:49.131915Z","shell.execute_reply":"2022-04-10T17:53:49.143885Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def simulation(y_test_t, y_pred, threshold=0.0):\n    \"\"\"This function simulates trading on based on the predictions with $200,000 and plots a graph representing each trade.\n    Trades are made when predicting a gain above a certain threshold.\n    The trades are plotted and compared to just holding the stock over the same time period.\n    \n    Arguments\n    ---------\n    y_test_t : torch.Tensor\n        Tensor corresponding to testing values to predict.\n    y_pred : torch.Tensor\n        The predictions made by the RNN.\n    threshold : float\n        Float corresponding to the percent gain used as a threshold. Trades are made when a value above this threshold is predicted.\n    \n    \"\"\"\n    \n    #Transforming tensors to numpy arrays.\n    y_test, y_pred = y_test_t.cpu().detach().numpy().flatten(), y_pred.cpu().detach().numpy().flatten()\n    \n    #Variables used.\n    initial = 200000\n    hold_profits = y_test\n    pred_profits = y_pred\n    \n    #Loop where we perform the trades.\n    for i in range(hold_profits.size):\n        if i == 0:\n            if pred_profits[i] > threshold:\n                pred_profits[i] = initial+y_test[i]*initial\n            else: \n                pred_profits[i] = initial\n            hold_profits[i] = initial+y_test[i]*initial\n        \n        else: \n            if pred_profits[i] > threshold:\n                pred_profits[i] = pred_profits[i-1] + pred_profits[i-1]*y_test[i]\n            else: \n                pred_profits[i] = pred_profits[i-1]\n            hold_profits[i] = hold_profits[i-1] + hold_profits[i-1]*y_test[i]\n    \n    #Plotting the Actual day-to-day %gain.\n    plt.figure(figsize=(30,10))\n    plt.plot(pred_profits, label=\"Total dollar amount using predictions \", color='orange')\n    plt.title('Total Dollar Amount over Time (starting with $200,000)')\n    plt.xlabel(\"time steps\")\n    plt.ylabel('total $')\n    plt.ticklabel_format(style='plain')\n    plt.legend(prop={'size': 10})\n    \n    #Plotting the Predicted day-to-day %gain.\n    plt.plot(hold_profits, label=\"Total dollar amount from holding\")\n    plt.legend()\n    \n    #Printing final profits from holding versus trading based on the predictions.\n    print(\"Total dollar amount from holding: \", hold_profits[-1])\n    print(\"Total dollarr amount from trading using prediction: \", pred_profits[-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.146235Z","iopub.execute_input":"2022-04-10T17:53:49.146599Z","iopub.status.idle":"2022-04-10T17:53:49.159882Z","shell.execute_reply.started":"2022-04-10T17:53:49.146545Z","shell.execute_reply":"2022-04-10T17:53:49.159121Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def plot_hyperparameter_search_results(params):\n    \"\"\"This function displays one 3d scatter plot where the x,y,z axes correspond \n    to the learning rate, hidden_size, and number of layers. \n    The darker points are the ones with the lower mse.\n    \n    Arguments\n    ---------\n    params : list\n        A list of dictionaries containing mse, learning rates, hidden sizes and number of layers.\n    \n    \"\"\"\n\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111, projection='3d')\n\n    values = [] \n    for i in params:\n        values.append(i['mse'])\n    # Mean mse will be used to remove outliers from the graph so we get a better picture of possible patterns.\n    mean_mse = np.mean(values)\n\n    mse_values = []\n    for i in params:\n        if i['mse'] < mean_mse:\n            mse_values.append(i['mse'])\n\n    lr_data = []\n    for i in params:\n        if i['mse'] < mean_mse:\n            lr_data.append(i['lr'])\n\n    hs_data = []\n    for i in params:\n        if i['mse'] < mean_mse:\n            hs_data.append(i['hidden_size'])\n\n    nl_data = []\n    for i in params:\n        if i['mse'] < mean_mse:\n            nl_data.append(i['num_layers'])\n\n    values = [] \n    for i in params:\n        if i['mse'] < mean_mse:\n            values.append(i['mse'])\n\n    #taking the log of the values so that differences are more apparent.\n    mse_values = np.log(mse_values)\n\n    img = ax.scatter(lr_data, hs_data, nl_data, c=mse_values, s=500, cmap=plt.hot(), alpha=1)\n    plt.xlabel('learning rate')\n    plt.ylabel('hidden size')\n    ax.set_zlabel('number of layers')\n    plt.title('MSE of RNN on Validation Set with Different Hyperparameters')\n    fig.colorbar(img)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.161146Z","iopub.execute_input":"2022-04-10T17:53:49.162669Z","iopub.status.idle":"2022-04-10T17:53:49.174785Z","shell.execute_reply.started":"2022-04-10T17:53:49.162630Z","shell.execute_reply":"2022-04-10T17:53:49.174102Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def plot_learning_rate_hyperparameter_search_results(params):\n    \"\"\"This function displays one 3d scatter plot for each learning rate. \n    The x,y,z axes correspond to the hidden_size, number of layers and the mse.\n    The darker points are the ones with the lower mse.\n    \n    Arguments\n    ---------\n    params : list\n        A list of dictionaries containing mse, learning rates, hidden sizes and number of layers.\n    \n    \"\"\"\n    \n    values = [] \n    for i in params:\n        values.append(i['mse'])\n    \n    # Mean mse will be used to remove outliers from the graph so we get a better picture of possible patterns.\n    mean_mse = np.mean(values)\n\n    mse_values = []\n    for i in params:\n        if i['mse'] < mean_mse:\n            mse_values.append(i['mse'])\n\n    lr_data = []\n    for i in params:\n        if i['mse'] < mean_mse:\n            lr_data.append(i['lr'])\n\n    hs_data = []\n    for i in params:\n        if i['mse'] < mean_mse:\n            hs_data.append(i['hidden_size'])\n\n    nl_data = []\n    for i in params:\n        if i['mse'] < mean_mse:\n            nl_data.append(i['num_layers'])\n\n    mse_data = [] \n    for i in params:\n        if i['mse'] < mean_mse:\n            mse_data.append(i['mse'])\n\n    lr_set = set(lr_data)\n    for l in lr_set:\n        mse_values = []\n        hs_values = []\n        nl_values = []\n        for i, lr in zip(range(len(lr_data)),lr_data):\n            if lr == l:\n                hs_values.append(hs_data[i])\n                nl_values.append(nl_data[i])\n                mse_values.append(mse_data[i])\n        hs_values = np.array(hs_values)\n        nl_values = np.array(nl_values)\n        mse_values = np.array(mse_values)\n        mse_values = mse_values[:, None]\n        fig = plt.figure(figsize=(10,10))\n        ax = fig.add_subplot(111, projection='3d')\n        img = ax.scatter(hs_values, nl_values, mse_values, c=mse_values, s=2000, cmap=plt.hot(), alpha=1)\n        plt.xlabel('hidden size')\n        plt.ylabel('number of layers')\n        ax.set_zlabel('mse')\n        plt.title(\"MSE of RNN with a Learning Rate of %.4f on Validation Set\" %(l))\n        fig.colorbar(img)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.176152Z","iopub.execute_input":"2022-04-10T17:53:49.176442Z","iopub.status.idle":"2022-04-10T17:53:49.190991Z","shell.execute_reply.started":"2022-04-10T17:53:49.176410Z","shell.execute_reply":"2022-04-10T17:53:49.190312Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Main Code","metadata":{}},{"cell_type":"markdown","source":"### Variables and seed initialization","metadata":{}},{"cell_type":"code","source":"#Using GPU for computation if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.192942Z","iopub.execute_input":"2022-04-10T17:53:49.193650Z","iopub.status.idle":"2022-04-10T17:53:49.245861Z","shell.execute_reply.started":"2022-04-10T17:53:49.193586Z","shell.execute_reply":"2022-04-10T17:53:49.244988Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#Set all seeds to 0 for reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.247222Z","iopub.execute_input":"2022-04-10T17:53:49.247691Z","iopub.status.idle":"2022-04-10T17:53:49.259015Z","shell.execute_reply.started":"2022-04-10T17:53:49.247644Z","shell.execute_reply":"2022-04-10T17:53:49.258209Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#Initializing all needed variables\n\n#Stock Ticker\nstock = 'AAPL'\n\n#Start and end date of dataset\nstart = '1990-01-01'\nend = '2022-01-01'\n\n#Percentage used to split the training and testing data. \nsplit_percentage = 0.8\n\n#Percentage used to split the training and validation data.\nvalidation_percentage = 0.9\n\n#Window shape is the number of days that we want each time slice to be (sequence length).\nwindow_shape = 30","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.260693Z","iopub.execute_input":"2022-04-10T17:53:49.261183Z","iopub.status.idle":"2022-04-10T17:53:49.266565Z","shell.execute_reply.started":"2022-04-10T17:53:49.261143Z","shell.execute_reply":"2022-04-10T17:53:49.265852Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Data importing and preprocessing","metadata":{}},{"cell_type":"code","source":"#Downloading the data and creating the dataset.\nfull_dataset = download_dataset(stock, start, end)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:49.267942Z","iopub.execute_input":"2022-04-10T17:53:49.268232Z","iopub.status.idle":"2022-04-10T17:53:51.420883Z","shell.execute_reply.started":"2022-04-10T17:53:49.268195Z","shell.execute_reply":"2022-04-10T17:53:51.420138Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#Split data into X and y sets.\nX, y = create_dataset(full_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:51.424411Z","iopub.execute_input":"2022-04-10T17:53:51.424951Z","iopub.status.idle":"2022-04-10T17:53:51.433362Z","shell.execute_reply.started":"2022-04-10T17:53:51.424907Z","shell.execute_reply":"2022-04-10T17:53:51.432532Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#Normalizing the data.\nX_norm, X_norm_mean, X_norm_std = mean_normalization(X, split_percentage)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:51.437714Z","iopub.execute_input":"2022-04-10T17:53:51.440428Z","iopub.status.idle":"2022-04-10T17:53:51.453535Z","shell.execute_reply.started":"2022-04-10T17:53:51.440393Z","shell.execute_reply":"2022-04-10T17:53:51.452921Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#Creating the sequences of window_shape days.\nX, y = create_sequences(X_norm, y, window_shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:51.457127Z","iopub.execute_input":"2022-04-10T17:53:51.457882Z","iopub.status.idle":"2022-04-10T17:53:51.463621Z","shell.execute_reply.started":"2022-04-10T17:53:51.457847Z","shell.execute_reply":"2022-04-10T17:53:51.462816Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#Splitting dataset into training and testing sets.\nX_train, y_train, X_val, y_val, X_test, y_test = split_dataset(X, y, split_percentage, validation_percentage)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:51.464903Z","iopub.execute_input":"2022-04-10T17:53:51.465509Z","iopub.status.idle":"2022-04-10T17:53:51.470357Z","shell.execute_reply.started":"2022-04-10T17:53:51.465457Z","shell.execute_reply":"2022-04-10T17:53:51.469277Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#Converting training and testing sets to tensors.\nX_train_t, y_train_t, X_val_t, y_val_t, X_test_t, y_test_t = dataset_to_tensors(X_train, y_train, X_val, y_val, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:51.475316Z","iopub.execute_input":"2022-04-10T17:53:51.475788Z","iopub.status.idle":"2022-04-10T17:53:51.536526Z","shell.execute_reply.started":"2022-04-10T17:53:51.475753Z","shell.execute_reply":"2022-04-10T17:53:51.535734Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#Making sure that the dataset is sent to the GPU if available.\nX_train_t=X_train_t.to(device)\ny_train_t=y_train_t.to(device)\nX_val_t=X_val_t.to(device)\ny_val_t=y_val_t.to(device)\nX_test_t=X_test_t.to(device)\ny_test_t=y_test_t.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:51.540164Z","iopub.execute_input":"2022-04-10T17:53:51.542151Z","iopub.status.idle":"2022-04-10T17:53:54.439345Z","shell.execute_reply.started":"2022-04-10T17:53:51.542111Z","shell.execute_reply":"2022-04-10T17:53:54.438613Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### RNN training","metadata":{}},{"cell_type":"code","source":"#Training the RNN without hyperparater search.\nrnn, losses = train_RNN(X_train_t, y_train_t, rnn_type = 'gru', lr = 0.0005, batch_size = 100, num_epoch = 1000, hidden_size = 10, num_layers = 2, dropout = 0.25)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:53:54.440668Z","iopub.execute_input":"2022-04-10T17:53:54.440931Z","iopub.status.idle":"2022-04-10T17:57:43.769223Z","shell.execute_reply.started":"2022-04-10T17:53:54.440897Z","shell.execute_reply":"2022-04-10T17:57:43.767545Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#Hyperparameters list\nrnn_type = 'gru'\nlr = [0.0005, 0.005, 0.05]\nhidden_size = [10, 20, 30]\nnum_layers = [1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:57:43.770406Z","iopub.execute_input":"2022-04-10T17:57:43.770678Z","iopub.status.idle":"2022-04-10T17:57:43.775819Z","shell.execute_reply.started":"2022-04-10T17:57:43.770641Z","shell.execute_reply":"2022-04-10T17:57:43.775108Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#Hyperparameter training.\n# rnn, mse_result, losses, params = hyperparameter_3d_search(X_train_t, y_train_t, X_val_t, y_val_t, rnn_type = rnn_type, lr = lr, hidden_size = hidden_size, num_layers = num_layers)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:57:43.777095Z","iopub.execute_input":"2022-04-10T17:57:43.778052Z","iopub.status.idle":"2022-04-10T17:57:43.786247Z","shell.execute_reply.started":"2022-04-10T17:57:43.778016Z","shell.execute_reply":"2022-04-10T17:57:43.785516Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#Plotting the losses.\nplot_losses(losses, 300)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:57:43.787275Z","iopub.execute_input":"2022-04-10T17:57:43.787603Z","iopub.status.idle":"2022-04-10T17:57:44.071161Z","shell.execute_reply.started":"2022-04-10T17:57:43.787548Z","shell.execute_reply":"2022-04-10T17:57:44.070488Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### RNN testing","metadata":{}},{"cell_type":"code","source":"#Performing the predictions on the test set.\ny_pred = predict(rnn, X_test_t)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:57:44.072172Z","iopub.execute_input":"2022-04-10T17:57:44.073718Z","iopub.status.idle":"2022-04-10T17:57:44.080232Z","shell.execute_reply.started":"2022-04-10T17:57:44.073678Z","shell.execute_reply":"2022-04-10T17:57:44.079502Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#Plotting the training predictions vs the training set percentage gain.\nseparate_plot_predicted_vs_actual_gains(y_train_t, predict(rnn, X_train_t))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:57:44.081402Z","iopub.execute_input":"2022-04-10T17:57:44.081895Z","iopub.status.idle":"2022-04-10T17:57:44.676693Z","shell.execute_reply.started":"2022-04-10T17:57:44.081832Z","shell.execute_reply":"2022-04-10T17:57:44.676005Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"#Plotting the predicted vs the test set percentage gain.\nsame_plot_predicted_vs_actual_gains(y_test_t, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:57:44.677867Z","iopub.execute_input":"2022-04-10T17:57:44.678451Z","iopub.status.idle":"2022-04-10T17:57:45.047075Z","shell.execute_reply.started":"2022-04-10T17:57:44.678413Z","shell.execute_reply":"2022-04-10T17:57:45.046428Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#Calculating the mean squared error of the predicted vs actual % gain on the test set.\nmse(y_test_t, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:57:45.048057Z","iopub.execute_input":"2022-04-10T17:57:45.048328Z","iopub.status.idle":"2022-04-10T17:57:45.108857Z","shell.execute_reply.started":"2022-04-10T17:57:45.048284Z","shell.execute_reply":"2022-04-10T17:57:45.107992Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"#Running a simulation on $200,000 to see how the RNN performs on the test set.\nsimulation(y_test_t, y_pred, threshold=0.0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:57:45.110204Z","iopub.execute_input":"2022-04-10T17:57:45.110599Z","iopub.status.idle":"2022-04-10T17:57:45.435094Z","shell.execute_reply.started":"2022-04-10T17:57:45.110546Z","shell.execute_reply":"2022-04-10T17:57:45.434442Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#Running a simulation on $200,000 to see how the RNN performs on the validation set.\nsimulation(y_val_t, predict(rnn, X_val_t), threshold=0.0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:01:39.362290Z","iopub.execute_input":"2022-04-10T18:01:39.362548Z","iopub.status.idle":"2022-04-10T18:01:39.657046Z","shell.execute_reply.started":"2022-04-10T18:01:39.362518Z","shell.execute_reply":"2022-04-10T18:01:39.656396Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Plotting","metadata":{}},{"cell_type":"code","source":"#Plotting the results of the hyperparamter search.\n# plot_hyperparameter_search_results(params)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:57:45.436263Z","iopub.execute_input":"2022-04-10T17:57:45.436875Z","iopub.status.idle":"2022-04-10T17:57:45.440592Z","shell.execute_reply.started":"2022-04-10T17:57:45.436836Z","shell.execute_reply":"2022-04-10T17:57:45.439635Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#Plotting the results of the hyperparameter search for each learning rate.\n# plot_learning_rate_hyperparameter_search_results(params)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T17:57:45.441713Z","iopub.execute_input":"2022-04-10T17:57:45.442540Z","iopub.status.idle":"2022-04-10T17:57:45.450057Z","shell.execute_reply.started":"2022-04-10T17:57:45.442504Z","shell.execute_reply":"2022-04-10T17:57:45.449368Z"},"trusted":true},"execution_count":40,"outputs":[]}]}